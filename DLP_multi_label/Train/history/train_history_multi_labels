3 training parameters
dynamic resampling, weight_power:0.7
custom loss function:
   class_weights  0.2
   ratio of negative and positive * 5


dynamic resampling
weights.txt:0.75 准确率  Multi_label_InceptionResNetV2-009-train0.9982_val0.9971.hdf5
但是倾向于 阴性

weights.txt修改为0.82没什么效果

dynamic resampling 结合 class_wights
weights.txt:0.74  get_weighted_loss  0:1,1:3
Multi_label_InceptionResNetV2-013-train0.9737_val0.946.hdf5
结果好了许多，但是小类依然
1 :
[[20568    17]
 [    8   444]]
2 :
[[20861    18]
 [    5   153]]
3 :
[[21003     6]
 [    2    26]]
4 :
[[19518   222]
 [   81  1216]]
5 :
[[20157    41]
 [   14   825]]
6 :
[[20814   103]
 [    4   116]]
7 :
[[20444    45]
 [   26   522]]
8 :
[[20844    83]
 [    9   101]]
9 :
[[20932    30]
 [    4    71]]
10 :
[[20207    56]
 [    6   768]]
11 :
[[20749   113]
 [    7   168]]
12 :
[[20898    24]
 [    2   113]]
13 :
[[21017     7]
 [    0    13]]
14 :
[[21010     5]
 [    7    15]]
15 :
[[20826    21]
 [    3   187]]
16 :
[[20940    16]
 [    0    81]]
17 :
[[20965     8]
 [    5    59]]
18 :
[[20953    11]
 [    0    73]]
19 :
[[21007     7]
 [    4    19]]
20 :
[[20964    23]
 [    0    50]]
21 :
[[19973   510]
 [   14   540]]
22 :
[[20997    10]
 [    0    30]]
23 :
[[20871    39]
 [    5   122]]
24 :
[[21003     8]
 [    3    23]]
25 :
[[20948    49]
 [    4    36]]
26 :
[[20460    19]
 [    4   554]]
27 :
[[20921    11]
 [    1   104]]
28 :
[[18353   223]
 [   50  2411]]

dynamic resampling 和 class_weights结合
weight_power:0.75
class_samples_weights = op_class_weight(LIST_CLASS_SAMPLES_NUM, weight_power=0.2)
class_samples_weights = np.array(class_samples_weights)
class_samples_weights = class_samples_weights * 4
class_weights1 = []
for _, class1 in enumerate(class_samples_weights):
    class_weights1.append([1, class1])

class_weights1 = np.array(class_weights1)

1 :
[[20559    26]
 [    5   447]]
2 :
[[20840    39]
 [    2   156]]
3 :
[[20999    10]
 [    1    27]]
4 :
[[19423   317]
 [   72  1225]]
5 :
[[20131    67]
 [    6   833]]
6 :
[[20747   170]
 [    1   119]]
7 :
[[20430    59]
 [   14   534]]
8 :
[[20726   201]
 [    6   104]]
9 :
[[20913    49]
 [    2    73]]
10 :
[[20183    80]
 [    3   771]]
11 :
[[20598   264]
 [    5   170]]
12 :
[[20878    44]
 [    2   113]]
13 :
[[21019     5]
 [    0    13]]
14 :
[[21008     7]
 [    1    21]]
15 :
[[20813    34]
 [    2   188]]
16 :
[[20918    38]
 [    0    81]]
17 :
[[20955    18]
 [    2    62]]
18 :
[[20940    24]
 [    0    73]]
19 :
[[21000    14]
 [    3    20]]
20 :
[[20945    42]
 [    0    50]]
21 :
[[19712   771]
 [    7   547]]
22 :
[[20981    26]
 [    0    30]]
23 :
[[20861    49]
 [    3   124]]
24 :
[[20989    22]
 [    3    23]]
25 :
[[20907    90]
 [    1    39]]
26 :
[[20455    24]
 [    1   557]]
27 :
[[20911    21]
 [    0   105]]
28 :
[[18238   338]
 [   27  2434]]


dynamic resampling 和 class_weights结合
weight_power:0.75
class_samples_weights = op_class_weight(LIST_CLASS_SAMPLES_NUM, weight_power=0.2)
class_samples_weights = np.array(class_samples_weights)
class_samples_weights = class_samples_weights * 5


2018_9_25
Xception Dilated output 17*17*1024
部分使用dilation_rate =1  weights=5
Multi_label_my_Xception-016-train0.9541_val0.933


2018_9_30
使用dilation_rate =2  weights=4
Multi_label_my_Xception-012-train0.9541_val0.937

 weights=5
Multi_label_my_Xception-016-train0.9508_val0.933


Total Data
weight=5
0.2
0.7

1 :
[[137112     96]
 [     3   3032]]
2 :
[[138958    210]
 [     1   1074]]
3 :
[[140040     17]
 [     3    183]]
4 :
[[129408   2116]
 [   270   8449]]
5 :
[[134225    453]
 [    25   5540]]
6 :
[[138412   1074]
 [     2    755]]
7 :
[[136290    406]
 [    24   3523]]
8 :
[[138404   1085]
 [     7    747]]
9 :
[[139385    312]
 [     4    542]]
10 :
[[134518    440]
 [    10   5275]]
11 :
[[138172   1076]
 [     3    992]]
12 :
[[139253    187]
 [     3    800]]
13 :
[[140122     20]
 [     0    101]]
14 :
[[140105     33]
 [     2    103]]
15 :
[[138698    118]
 [     5   1422]]
16 :
[[139497    199]
 [     0    547]]
17 :
[[139744     74]
 [     1    424]]
18 :
[[139691     78]
 [     1    473]]
19 :
[[139958     98]
 [     4    183]]
20 :
[[139654    216]
 [     0    373]]
21 :
[[131790   4642]
 [    19   3792]]
22 :
[[139937    119]
 [     0    187]]
23 :
[[139161    308]
 [     2    772]]
24 :
[[139976    102]
 [     2    163]]
25 :
[[139460    516]
 [     0    267]]
26 :
[[136377    157]
 [     2   3707]]
27 :
[[139464     95]
 [     0    684]]
28 :
[[121740   2008]
 [   102  16393]]